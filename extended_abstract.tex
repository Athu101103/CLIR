\begin{abstract}
The preservation and accessibility of Sanskrit literature represents one of humanity's most pressing digital humanities challenges. With over three millennia of philosophical, scientific, and cultural knowledge encoded in Sanskrit texts, the language barrier prevents global scholars from accessing this vast repository of ancient wisdom. Cross-Lingual Information Retrieval (CLIR) systems offer a pathway to democratize Sanskrit knowledge by enabling English-language queries to discover relevant Sanskrit documents. However, existing CLIR approaches fail catastrophically when applied to Sanskrit due to fundamental linguistic and cultural barriers that distinguish ancient languages from their modern counterparts.

This research addresses the unique challenges of Sanskrit-English CLIR through a novel zero-shot framework that operates without parallel Sanskrit-English training data—a critical advantage given the scarcity of such resources. Our approach recognizes that Sanskrit's complexity extends beyond mere linguistic differences to encompass profound cultural and philosophical concepts that resist direct translation. Terms like "dharma" (righteous duty), "moksha" (liberation), and "samsara" (cycle of existence) carry layered meanings that evolve across different philosophical schools and historical periods, creating semantic challenges that conventional CLIR systems cannot address.

The cornerstone of our methodology lies in exploiting the shared Devanagari script heritage among Sanskrit, Hindi, Marathi, and Nepali languages. While these languages have diverged significantly in vocabulary and grammar over centuries, they retain structural similarities in morphological patterns, conjunct consonant formations, and syntactic constructions that our cross-script transfer learning mechanism leverages. We develop a sophisticated normalization pipeline that standardizes Devanagari representations across languages, followed by semantic alignment learning that maps cross-lingual conceptual relationships. This transfer mechanism enables our system to bootstrap Sanskrit understanding from the relatively abundant Hindi, Marathi, and Nepali parallel corpora, achieving effective knowledge transfer despite the temporal and stylistic gaps between classical Sanskrit and contemporary Indic languages.

Recognizing that cultural context plays a crucial role in Sanskrit text interpretation, we integrate large language models through carefully designed few-shot prompting strategies. Our approach curates five exemplar Sanskrit-English pairs that span core philosophical domains: consciousness studies ("चैतन्यं ब्रह्म" - consciousness as ultimate reality), ethical frameworks ("धर्मः धारयते जगत्" - righteousness sustains the universe), spiritual practices ("योगश्चित्तवृत्तिनिरोधः" - yoga as mental discipline), liberation philosophy ("मोक्षः सर्वदुःखनिवृत्तिः" - freedom from suffering), and action philosophy ("कर्मण्येवाधिकारस्ते" - right to action alone). These exemplars provide contextual anchors that guide the language model's understanding of Sanskrit philosophical discourse, enabling nuanced relevance assessments that capture cultural and conceptual subtleties.

To ensure computational efficiency and practical deployment feasibility, we employ Low-Rank Adaptation (LoRA) for parameter-efficient fine-tuning. This approach reduces computational overhead by 80% compared to full model fine-tuning while maintaining competitive performance. Our LoRA implementation targets critical transformer components—query, key, value, and dense projection layers—with rank-16 decomposition, enabling effective model specialization for Sanskrit-English retrieval tasks without prohibitive resource requirements.

Comprehensive evaluation on the Anveshana benchmark dataset, containing 3,400 Sanskrit-English query-document pairs extracted from Srimadbhagavatam, demonstrates the effectiveness of our integrated approach. Cross-script transfer learning alone achieves 47.9% improvement in NDCG@10 scores over baseline XLM-RoBERTa, rising from 0.3245 to 0.4799. Few-shot learning with philosophical exemplars yields 56.0% enhancement, reaching 0.5063 NDCG@10. Most significantly, our complete framework achieves 80.2% improvement, attaining 0.5847 NDCG@10, establishing new performance benchmarks for ancient language CLIR systems.

Beyond quantitative metrics, our framework addresses fundamental challenges in digital humanities by providing a scalable approach for ancient language processing. The methodology's emphasis on cultural context preservation and computational efficiency makes it applicable to other ancient languages with limited parallel resources, including Classical Greek, Latin, and ancient Chinese texts. This work represents a crucial step toward democratizing access to humanity's ancient knowledge repositories, enabling cross-cultural scholarly collaboration and preserving cultural heritage for future generations through modern information retrieval technologies.
\end{abstract}