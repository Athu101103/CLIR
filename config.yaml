# Main configuration for Anveshana CLIR implementation

# Dataset Configuration
dataset:
  name: "manojbalaji1/anveshana"
  train_split: 0.9
  validation_split: 0.1
  test_size: 340
  negative_sampling_ratio: 2  # 2:1 negative to positive ratio

# Model Configurations
models:
  # Query Translation (QT) Models
  qt:
    bm25:
      k1: 1.2
      b: 0.75
    xlm_roberta:
      model_name: "xlm-roberta-base"
      max_length: 512
      batch_size: 16
      learning_rate: 2e-5
      epochs: 3
  
  # Document Translation (DT) Models
  dt:
    bm25:
      k1: 1.2
      b: 0.75
    contriever:
      model_name: "mjwong/contriever-mnli"
      max_length: 512
      batch_size: 16
      learning_rate: 2e-5
      epochs: 3
    colbert:
      model_name: "colbert-ai/colbert-base"
      max_length: 512
      batch_size: 8
      learning_rate: 1e-5
      epochs: 3
    gpt2:
      model_name: "gpt2"
      max_length: 512
      batch_size: 8
      learning_rate: 5e-5
      epochs: 3
    replug:
      retriever_model: "contriever"
      language_model: "gpt2"
      supervision_model: "mistral-7b-instruct-v0.2"
      top_k_retrieval: 20
  
  # Direct Retrieve (DR) Models
  dr:
    mdpr:
      model_name: "xlm-roberta-base"
      max_length: 512
      batch_size: 16
      learning_rate: 2e-5
      epochs: 3
    xlm_roberta:
      model_name: "xlm-roberta-base"
      max_length: 512
      batch_size: 16
      learning_rate: 2e-5
      epochs: 3
    multilingual_e5:
      model_name: "intfloat/multilingual-e5-base"
      max_length: 512
      batch_size: 16
      learning_rate: 2e-5
      epochs: 3
    gpt2:
      model_name: "gpt2"
      max_length: 512
      batch_size: 8
      learning_rate: 5e-5
      epochs: 3
  
  # Zero-shot Models
  zero_shot:
    colbert:
      model_name: "colbert-ai/colbert-base"
    xlm_roberta:
      model_name: "xlm-roberta-base"
    contriever:
      model_name: "facebook/contriever"
    multilingual_e5:
      model_name: "intfloat/multilingual-e5-base"

# Translation Configuration
translation:
  google_translate:
    batch_size: 50
    retry_attempts: 3
    delay_between_requests: 1.0

# Evaluation Configuration
evaluation:
  metrics: ["ndcg", "map", "recall", "precision"]
  k_values: [1, 3, 5, 10]
  faiss:
    index_type: "IndexFlatIP"  # Inner Product for cosine similarity
    nprobe: 10

# Training Configuration
training:
  seed: 42
  device: "cuda"  # or "cpu"
  num_workers: 4
  gradient_accumulation_steps: 1
  warmup_steps: 100
  weight_decay: 0.01
  max_grad_norm: 1.0

# Logging and Output
logging:
  log_level: "INFO"
  save_checkpoints: true
  checkpoint_dir: "checkpoints"
  results_dir: "results"
  wandb_project: "anveshana-clir"

# Data Processing
preprocessing:
  sanskrit:
    remove_non_devanagari: true
    handle_poetic_structures: true
    poetic_marker_replacement: "——"
  english:
    minimal_preprocessing: true 