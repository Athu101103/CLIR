# Advanced Zero-Shot Learning for Sanskrit-English CLIR

This module implements cutting-edge zero-shot and few-shot learning techniques for Cross-Lingual Information Retrieval between English and Sanskrit, as requested in the research enhancement proposal.

## üî• Key Features Implemented

### 1. Cross-Script Transfer Learning
- **Devanagari Script Normalization**: Unified processing across Hindi, Marathi, Nepali, and Sanskrit
- **Multi-Source Transfer**: Learn from multiple Devanagari-script languages
- **Feature Extraction**: Extract transferable cross-lingual features
- **Alignment Learning**: Compute semantic alignments between languages

### 2. Multilingual Prompting with Large Language Models
- **Few-Shot Prompting**: Create context-aware prompts for Sanskrit understanding
- **In-Context Learning**: Leverage LLM knowledge for Sanskrit-English alignment
- **Task-Specific Prompts**: Customizable prompts for retrieval and translation tasks
- **Response Generation**: Generate explanations and similarity scores

### 3. Parameter-Efficient Fine-Tuning
- **LoRA Integration**: Low-Rank Adaptation for efficient fine-tuning
- **Adapter Methods**: Parameter-efficient adaptation layers
- **Contrastive Learning**: Specialized loss functions for similarity learning
- **Memory Efficient**: Minimal parameter updates while maintaining performance

### 4. Advanced Model Architecture
- **Hybrid Approach**: Combines multiple techniques for optimal performance
- **Modular Design**: Enable/disable individual components
- **Configurable**: Extensive configuration options
- **Scalable**: Designed for production deployment

## üìÅ File Structure

```
src/zero_shot_framework/
‚îú‚îÄ‚îÄ advanced_zs_learning.py      # Main implementation
‚îú‚îÄ‚îÄ advanced_zs_runner.py        # Experiment runner
‚îú‚îÄ‚îÄ base_zs.py                   # Base classes (existing)
‚îî‚îÄ‚îÄ [other existing files]

examples/
‚îî‚îÄ‚îÄ advanced_zs_example.py       # Usage example

test_advanced_zs.py              # Test suite
ADVANCED_ZS_README.md            # This documentation
```

## üöÄ Quick Start

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. Run Tests
```bash
python test_advanced_zs.py
```

### 3. Run Example
```bash
python examples/advanced_zs_example.py
```

### 4. Run Full Experiments
```bash
python src/zero_shot_framework/advanced_zs_runner.py
```

## üîß Configuration

### Basic Configuration
```python
config = {
    'model_name': 'xlm-roberta-base',
    'source_languages': ['hindi', 'marathi', 'nepali'],
    'lora_r': 16,
    'lora_alpha': 32,
    'lora_dropout': 0.1,
    'llm_model': 'facebook/mbart-large-50-many-to-many-mmt'
}
```

### Advanced Configuration Options
- **Cross-Script Transfer**: Configure source languages and similarity thresholds
- **LoRA Parameters**: Adjust rank, alpha, dropout for fine-tuning
- **LLM Selection**: Choose between different multilingual language models
- **Technique Combination**: Enable/disable individual components

## üí° Usage Examples

### Basic Usage
```python
from src.zero_shot_framework.advanced_zs_learning import AdvancedZeroShotModel

# Initialize model
model = AdvancedZeroShotModel(config)

# Retrieve documents
scores = model.retrieve_documents(
    queries=["What is dharma?"],
    documents=["‡§ß‡§∞‡•ç‡§Æ‡§É ‡§ß‡§æ‡§∞‡§Ø‡§§‡•á ‡§ú‡§ó‡§§‡•ç"],
    k=10
)
```

### Cross-Script Transfer
```python
# Enable cross-script transfer from Hindi/Marathi
scores = model.retrieve_documents(
    queries, documents, k=10,
    use_cross_script=True,
    use_few_shot=False
)
```

### Few-Shot Learning
```python
from src.zero_shot_framework.advanced_zs_learning import create_few_shot_examples

examples = create_few_shot_examples()
scores = model.retrieve_documents(
    queries, documents, k=10,
    use_cross_script=False,
    use_few_shot=True,
    few_shot_examples=examples
)
```

### Combined Techniques
```python
# Use all advanced techniques together
scores = model.retrieve_documents(
    queries, documents, k=10,
    use_cross_script=True,
    use_few_shot=True,
    few_shot_examples=examples
)
```

### Parameter-Efficient Fine-Tuning
```python
# Fine-tune with LoRA
model.parameter_efficient_fine_tune(
    train_queries, train_documents, train_labels,
    output_dir="./lora_checkpoints"
)
```

## üß™ Experimental Results

The implementation supports comprehensive evaluation across multiple techniques:

### Experiment Types
1. **Cross-Script Transfer**: Evaluate transfer learning effectiveness
2. **Few-Shot Learning**: Assess in-context learning capabilities
3. **Combined Techniques**: Test synergistic effects
4. **Parameter-Efficient Fine-Tuning**: Demonstrate LoRA effectiveness

### Metrics
- NDCG@k (1, 3, 5, 10)
- MAP@k (1, 3, 5, 10)
- Recall@k (1, 3, 5, 10)
- Precision@k (1, 3, 5, 10)

### Sample Results Format
```json
{
  "cross_script_transfer": {
    "model_name": "Advanced ZS - Cross-Script Transfer",
    "evaluation_results": {
      "ndcg": {"1": 0.45, "3": 0.52, "5": 0.58, "10": 0.62},
      "map": {"1": 0.45, "3": 0.48, "5": 0.51, "10": 0.53}
    }
  }
}
```

## üî¨ Technical Details

### Cross-Script Transfer Learning
- **Script Normalization**: Unified Devanagari character processing
- **Feature Alignment**: Learn cross-lingual semantic mappings
- **Transfer Methods**: Multiple source language support
- **Similarity Computation**: Advanced alignment algorithms

### Multilingual Prompting
- **Context Creation**: Generate informative few-shot prompts
- **LLM Integration**: Support for multiple language models
- **Response Parsing**: Extract relevance scores from LLM outputs
- **Batch Processing**: Efficient prompt-based scoring

### Parameter-Efficient Fine-Tuning
- **LoRA Implementation**: Low-rank matrix decomposition
- **Target Modules**: Configurable attention and feedforward layers
- **Contrastive Loss**: Custom loss function for similarity learning
- **Memory Optimization**: Minimal parameter overhead

### Model Architecture
```
AdvancedZeroShotModel
‚îú‚îÄ‚îÄ CrossScriptTransferLearner
‚îÇ   ‚îú‚îÄ‚îÄ DevanagariScriptNormalizer
‚îÇ   ‚îî‚îÄ‚îÄ Feature extraction & alignment
‚îú‚îÄ‚îÄ LLMPromptingEngine
‚îÇ   ‚îú‚îÄ‚îÄ Prompt generation
‚îÇ   ‚îî‚îÄ‚îÄ Response parsing
‚îú‚îÄ‚îÄ ParameterEfficientFineTuner
‚îÇ   ‚îú‚îÄ‚îÄ LoRA configuration
‚îÇ   ‚îî‚îÄ‚îÄ Contrastive training
‚îî‚îÄ‚îÄ Base transformer model
```

## üìä Performance Expectations

Based on the research literature and our implementation:

### Expected Improvements
- **Cross-Script Transfer**: 5-15% improvement over baseline
- **Few-Shot Learning**: 10-20% improvement with good examples
- **Combined Techniques**: 15-25% improvement over standard zero-shot
- **Parameter-Efficient Fine-Tuning**: 20-30% improvement with domain data

### Computational Efficiency
- **Memory Usage**: 20-50% reduction vs full fine-tuning
- **Training Time**: 10x faster than full model fine-tuning
- **Inference Speed**: Minimal overhead for advanced features

## üõ†Ô∏è Troubleshooting

### Common Issues

1. **Model Loading Errors**
   ```bash
   # Solution: Use smaller models for testing
   config['model_name'] = 'distilbert-base-multilingual-cased'
   config['llm_model'] = 'google/mt5-small'
   ```

2. **Memory Issues**
   ```bash
   # Solution: Reduce batch sizes and sequence lengths
   # Use CPU instead of GPU if necessary
   ```

3. **Dependency Issues**
   ```bash
   # Solution: Install specific versions
   pip install peft==0.4.0
   pip install transformers==4.30.0
   ```

### Performance Optimization

1. **For Fast Testing**: Use `distilbert-base-multilingual-cased`
2. **For Best Performance**: Use `xlm-roberta-large`
3. **For Memory Efficiency**: Enable gradient checkpointing
4. **For Speed**: Use mixed precision training

## üîÆ Future Enhancements

### Planned Features
1. **Multi-Modal Integration**: Add visual Sanskrit manuscript processing
2. **Temporal Adaptation**: Handle different periods of Sanskrit literature
3. **Domain Specialization**: Create domain-specific models
4. **Interactive Learning**: Real-time feedback incorporation

### Research Opportunities
1. **Novel Architectures**: Sanskrit-specific attention mechanisms
2. **Cultural Context**: Integrate philosophical and cultural knowledge
3. **Evaluation Metrics**: Develop Sanskrit-specific evaluation methods
4. **Corpus Expansion**: Create larger parallel Sanskrit-English datasets

## üìö References

1. **LoRA**: Hu et al. "LoRA: Low-Rank Adaptation of Large Language Models"
2. **Cross-Lingual Transfer**: Conneau et al. "Unsupervised Cross-lingual Representation Learning"
3. **Few-Shot Learning**: Brown et al. "Language Models are Few-Shot Learners"
4. **Sanskrit NLP**: Krishna et al. "A Graph-Based Framework for Structured Prediction Tasks in Sanskrit"

## ü§ù Contributing

1. **Code Style**: Follow PEP 8 guidelines
2. **Testing**: Add tests for new features
3. **Documentation**: Update docstrings and README
4. **Experiments**: Include evaluation results

## üìÑ License

This implementation follows the project's main license and is designed for research purposes.

---

**Note**: This advanced zero-shot learning implementation represents a significant enhancement to the base Anveshana CLIR system, incorporating state-of-the-art techniques for improved Sanskrit-English cross-lingual information retrieval.